{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be609eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848bcf154dab052f",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "import PyPDF2\n",
    "\n",
    "# Function to read and preprocess PDF text\n",
    "def process_pdf(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        pdf_reader = PyPDF2.PdfReader(f)\n",
    "        text = \"\"\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text()\n",
    "        return clean_wiki_text(text)\n",
    "\n",
    "# Initialize question answering pipeline\n",
    "qa_pipeline = pipeline(\"question-answering\")\n",
    "\n",
    "# Load your PDF document\n",
    "pdf_text = process_pdf(\"your_trade_document.pdf\")\n",
    "\n",
    "# Example usage\n",
    "user_question = \"What is the import duty for HS code 12345 mentioned in the document?\"\n",
    "answer = qa_pipeline(questions=[user_question], context=pdf_text)[\"answers\"][0]\n",
    "\n",
    "print(f\"Answer: {answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a760dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q cassandra-driver \n",
    "!pip install -q cassio>=0.1.1 \n",
    "!pip install -q gradientai --upgrade \n",
    "!pip install -q llama-index \n",
    "!pip install -q pypdf \n",
    "!pip install -q tiktoken==0.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cae74957ceb64a7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import streamlit as st\n",
    "from cassandra.auth import PlainTextAuthProvider\n",
    "from cassandra.cluster import Cluster\n",
    "from llama_index import ServiceContext\n",
    "from llama_index import set_global_service_context\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, StorageContext\n",
    "from llama_index.embeddings import GradientEmbedding\n",
    "from llama_index.llms import GradientBaseModelLLM\n",
    "from llama_index.vector_stores import CassandraVectorStore\n",
    "from copy import deepcopy\n",
    "from tempfile import NamedTemporaryFile\n",
    "\n",
    "@st.cache_resource\n",
    "def create_datastax_connection():\n",
    "\n",
    "    cloud_config= {'secure_connect_bundle': r\"C:\\Users\\PRKUMAR\\Downloads\\secure-connect-nlp.zip\"}\n",
    "\n",
    "    with open(r\"C:\\Users\\PRKUMAR\\Downloads\\premkumarc1111@gmail.com-token.json\") as f:\n",
    "        secrets = json.load(f)\n",
    "\n",
    "    CLIENT_ID = secrets[\"clientId\"]\n",
    "    CLIENT_SECRET = secrets[\"secret\"]\n",
    "\n",
    "    auth_provider = PlainTextAuthProvider(CLIENT_ID, CLIENT_SECRET)\n",
    "    cluster = Cluster(cloud=cloud_config, auth_provider=auth_provider)\n",
    "    astra_session = cluster.connect()\n",
    "    return astra_session\n",
    "\n",
    "def main():\n",
    "\n",
    "    index_placeholder = None\n",
    "    st.set_page_config(page_title = \"Chat with your PDF using Llama2 & Llama Index\", page_icon=\"ü¶ô\")\n",
    "    st.header('ü¶ô Chat with your PDF using Llama2 model & Llama Index')\n",
    "    \n",
    "    if \"conversation\" not in st.session_state:\n",
    "        st.session_state.conversation = None\n",
    "\n",
    "    if \"activate_chat\" not in st.session_state:\n",
    "        st.session_state.activate_chat = False\n",
    "\n",
    "    if \"messages\" not in st.session_state:\n",
    "        st.session_state.messages = []\n",
    "\n",
    "    for message in st.session_state.messages:\n",
    "        with st.chat_message(message[\"role\"], avatar = message['avatar']):\n",
    "            st.markdown(message[\"content\"])\n",
    "\n",
    "    session = create_datastax_connection()\n",
    "\n",
    "    os.environ['GRADIENT_ACCESS_TOKEN'] = \"CK1zDOU4BQ03NoMJMUDndWM8oAoNFMpm\"\n",
    "    os.environ['GRADIENT_WORKSPACE_ID'] = \"55708ca1-7b2b-42b1-ae1b-7d75ef4bc8c7_workspace\"\n",
    "\n",
    "    llm = GradientBaseModelLLM(base_model_slug=\"llama2-7b-chat\", max_tokens=400)\n",
    "\n",
    "    embed_model = GradientEmbedding(\n",
    "        gradient_access_token = os.environ[\"GRADIENT_ACCESS_TOKEN\"],\n",
    "        gradient_workspace_id = os.environ[\"GRADIENT_WORKSPACE_ID\"],\n",
    "        gradient_model_slug=\"bge-large\")\n",
    "\n",
    "    service_context = ServiceContext.from_defaults(\n",
    "    llm = llm,\n",
    "    embed_model = embed_model,\n",
    "    chunk_size=256)\n",
    "\n",
    "    set_global_service_context(service_context)\n",
    "\n",
    "    with st.sidebar:\n",
    "        st.subheader('Upload Your PDF File')\n",
    "        docs = st.file_uploader('‚¨ÜÔ∏è Upload your PDF & Click to process',\n",
    "                                accept_multiple_files = False, \n",
    "                                type=['pdf'])\n",
    "        if st.button('Process'):\n",
    "            with NamedTemporaryFile(dir='.', suffix='.pdf') as f:\n",
    "                f.write(docs.getbuffer())\n",
    "                with st.spinner('Processing'):\n",
    "                    documents = SimpleDirectoryReader(\".\").load_data()\n",
    "                    index = VectorStoreIndex.from_documents(documents,\n",
    "                                                            service_context=service_context)\n",
    "                    query_engine = index.as_query_engine()\n",
    "                    if \"query_engine\" not in st.session_state:\n",
    "                        st.session_state.query_engine = query_engine\n",
    "                    st.session_state.activate_chat = True\n",
    "\n",
    "    if st.session_state.activate_chat == True:\n",
    "        if prompt := st.chat_input(\"Ask your question from the PDF?\"):\n",
    "            with st.chat_message(\"user\", avatar = 'üë®üèª'):\n",
    "                st.markdown(prompt)\n",
    "            st.session_state.messages.append({\"role\": \"user\", \n",
    "                                              \"avatar\" :'üë®üèª',\n",
    "                                              \"content\": prompt})\n",
    "\n",
    "            query_index_placeholder = st.session_state.query_engine\n",
    "            pdf_response = query_index_placeholder.query(prompt)\n",
    "            cleaned_response = pdf_response.response\n",
    "            with st.chat_message(\"assistant\", avatar='ü§ñ'):\n",
    "                st.markdown(cleaned_response)\n",
    "            st.session_state.messages.append({\"role\": \"assistant\", \n",
    "                                              \"avatar\" :'ü§ñ',\n",
    "                                              \"content\": cleaned_response})\n",
    "        else:\n",
    "            st.markdown(\n",
    "                'Upload your PDFs to chat'\n",
    "                )\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f48beb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = 'your_openai_api_key'\n",
    "\n",
    "from llama_index.core import ServiceContext, set_global_service_context\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.llms.gradient import GradientBaseModelLLM\n",
    "from llama_index.embeddings.openai import OpenAIGPTEmbedding\n",
    "\n",
    "# Set up OpenAI API endpoint\n",
    "api_key = os.environ['OPENAI_API_KEY']\n",
    "openai_endpoint = \"https://api.openai.com/v1/engines/davinci-codex/completions\"\n",
    "\n",
    "llm = GradientBaseModelLLM(\n",
    "    base_model_slug=\"llama2-7b-chat\",\n",
    "    max_tokens=400,\n",
    ")\n",
    "\n",
    "embed_model = OpenAIGPTEmbedding(api_key=api_key)\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm,\n",
    "    embed_model=embed_model,\n",
    "    chunk_size=256,\n",
    ")\n",
    "\n",
    "set_global_service_context(service_context)\n",
    "\n",
    "documents = SimpleDirectoryReader(\"docs\").load_data()\n",
    "print(f\"Loaded {len(documents)} document(s).\")\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "# Example query using GPT-4 via OpenAI API\n",
    "query = \"what is the payment history\"\n",
    "response = requests.post(openai_endpoint, json={\"prompt\": query, \"max_tokens\": 100})\n",
    "response_json = response.json()\n",
    "print(response_json)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
